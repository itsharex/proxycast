//! ASR 服务
//!
//! 统一管理语音识别服务，支持本地 Whisper 和云端 ASR。
//!
//! ## 功能
//! - 本地 Whisper 识别（离线、隐私）
//! - OpenAI Whisper API
//! - 百度语音识别
//! - 讯飞语音识别（WebSocket 流式）
//!
//! ## 模型文件路径
//! Whisper 模型文件存储在：`~/Library/Application Support/proxycast/models/whisper/`
//!
//! 支持的模型：
//! - `ggml-tiny.bin` (~75MB)
//! - `ggml-base.bin` (~142MB)
//! - `ggml-small.bin` (~466MB)
//! - `ggml-medium.bin` (~1.5GB)
//!
//! ## 使用示例
//! ```rust,ignore
//! let credential = AsrService::get_default_credential()?.unwrap();
//! let text = AsrService::transcribe(&credential, &audio_data, 16000).await?;
//! ```

#[cfg(feature = "local-whisper")]
use std::path::PathBuf;

#[cfg(feature = "local-whisper")]
use crate::config::WhisperModelSize;
use crate::config::{load_config, AsrCredentialEntry, AsrProviderType};
use voice_core::asr_client::{AsrClient, BaiduClient, OpenAIWhisperClient, XunfeiClient};
use voice_core::types::AudioData;

/// ASR 服务
pub struct AsrService;

impl AsrService {
    /// 获取默认 ASR 凭证
    pub fn get_default_credential() -> Result<Option<AsrCredentialEntry>, String> {
        let config = load_config().map_err(|e| e.to_string())?;
        Ok(config
            .credential_pool
            .asr
            .into_iter()
            .find(|c| c.is_default && !c.disabled))
    }

    /// 获取指定 ID 的 ASR 凭证
    pub fn get_credential(id: &str) -> Result<Option<AsrCredentialEntry>, String> {
        let config = load_config().map_err(|e| e.to_string())?;
        Ok(config.credential_pool.asr.into_iter().find(|c| c.id == id))
    }

    /// 使用指定凭证进行语音识别
    ///
    /// 当云端服务失败时，自动回退到本地 Whisper（需求 3.4）
    pub async fn transcribe(
        credential: &AsrCredentialEntry,
        audio_data: &[u8],
        sample_rate: u32,
    ) -> Result<String, String> {
        // 如果是本地 Whisper，直接调用
        if matches!(credential.provider, AsrProviderType::WhisperLocal) {
            return Self::transcribe_whisper_local(credential, audio_data, sample_rate).await;
        }

        // 云端服务：先尝试云端，失败则回退到本地 Whisper
        let cloud_result = match credential.provider {
            AsrProviderType::OpenAI => {
                Self::transcribe_openai(credential, audio_data, sample_rate).await
            }
            AsrProviderType::Baidu => {
                Self::transcribe_baidu(credential, audio_data, sample_rate).await
            }
            AsrProviderType::Xunfei => {
                Self::transcribe_xunfei(credential, audio_data, sample_rate).await
            }
            AsrProviderType::WhisperLocal => unreachable!(), // 已在上面处理
        };

        // 云端成功，直接返回
        if cloud_result.is_ok() {
            return cloud_result;
        }

        // 云端失败，尝试回退到本地 Whisper
        let cloud_error = cloud_result.unwrap_err();
        tracing::warn!(
            "云端 ASR 服务 ({:?}) 失败: {}，尝试回退到本地 Whisper",
            credential.provider,
            cloud_error
        );

        // 尝试获取本地 Whisper 凭证
        match Self::get_whisper_local_credential() {
            Ok(Some(whisper_credential)) => {
                tracing::info!("正在使用本地 Whisper 进行回退识别...");
                match Self::transcribe_whisper_local(&whisper_credential, audio_data, sample_rate)
                    .await
                {
                    Ok(text) => {
                        tracing::info!("本地 Whisper 回退识别成功");
                        Ok(text)
                    }
                    Err(whisper_error) => {
                        tracing::error!("本地 Whisper 回退也失败: {}", whisper_error);
                        // 返回原始云端错误，因为那是用户选择的服务
                        Err(format!(
                            "云端服务失败: {cloud_error}；本地 Whisper 回退也失败: {whisper_error}"
                        ))
                    }
                }
            }
            Ok(None) => {
                tracing::warn!("未找到本地 Whisper 凭证，无法回退");
                Err(format!(
                    "云端服务失败: {cloud_error}；未配置本地 Whisper，无法回退"
                ))
            }
            Err(e) => {
                tracing::error!("获取本地 Whisper 凭证失败: {}", e);
                Err(format!(
                    "云端服务失败: {cloud_error}；获取本地 Whisper 凭证失败: {e}"
                ))
            }
        }
    }

    /// 获取本地 Whisper 凭证（用于回退）
    fn get_whisper_local_credential() -> Result<Option<AsrCredentialEntry>, String> {
        let config = load_config().map_err(|e| e.to_string())?;
        Ok(config
            .credential_pool
            .asr
            .into_iter()
            .find(|c| matches!(c.provider, AsrProviderType::WhisperLocal) && !c.disabled))
    }

    /// 本地 Whisper 识别
    #[cfg(feature = "local-whisper")]
    async fn transcribe_whisper_local(
        credential: &AsrCredentialEntry,
        audio_data: &[u8],
        sample_rate: u32,
    ) -> Result<String, String> {
        // 获取 Whisper 配置
        let whisper_config = credential
            .whisper_config
            .as_ref()
            .ok_or("Whisper 本地配置缺失")?;

        // 获取模型文件路径
        let model_path = Self::get_whisper_model_path(&whisper_config.model)?;

        // 将 PCM 字节转换为 i16 采样
        let audio = Self::build_audio_data(audio_data, sample_rate)?;

        // 检查录音时长
        if !audio.is_valid() {
            return Err("录音时间过短（需要至少 0.5 秒）".to_string());
        }

        // 转换模型大小枚举
        let model = Self::convert_model_size(&whisper_config.model);

        // 创建 Whisper 识别器
        let transcriber =
            voice_core::WhisperTranscriber::new(model_path, model, &credential.language)
                .map_err(|e| format!("Whisper 模型加载失败: {e}"))?;

        // 执行识别
        let result = transcriber
            .transcribe(&audio)
            .map_err(|e| format!("Whisper 识别失败: {e}"))?;

        Ok(result.text)
    }

    /// 本地 Whisper 识别（未启用 local-whisper feature 时的 stub）
    #[cfg(not(feature = "local-whisper"))]
    async fn transcribe_whisper_local(
        _credential: &AsrCredentialEntry,
        _audio_data: &[u8],
        _sample_rate: u32,
    ) -> Result<String, String> {
        Err("本地 Whisper 功能未启用。请使用云端 ASR 服务（OpenAI、百度、讯飞）".to_string())
    }

    /// 获取 Whisper 模型文件路径
    #[cfg(feature = "local-whisper")]
    fn get_whisper_model_path(model_size: &WhisperModelSize) -> Result<PathBuf, String> {
        // 模型文件名
        let filename = match model_size {
            WhisperModelSize::Tiny => "ggml-tiny.bin",
            WhisperModelSize::Base => "ggml-base.bin",
            WhisperModelSize::Small => "ggml-small.bin",
            WhisperModelSize::Medium => "ggml-medium.bin",
        };

        // 模型存储目录：~/Library/Application Support/proxycast/models/whisper/
        let models_dir = dirs::data_dir()
            .ok_or("无法获取数据目录")?
            .join("proxycast")
            .join("models")
            .join("whisper");

        let model_path = models_dir.join(filename);

        // 检查模型文件是否存在
        if !model_path.exists() {
            return Err(format!(
                "Whisper 模型文件不存在: {}\n请下载模型文件到: {}",
                filename,
                models_dir.display()
            ));
        }

        Ok(model_path)
    }

    /// 转换模型大小枚举
    #[cfg(feature = "local-whisper")]
    fn convert_model_size(size: &WhisperModelSize) -> voice_core::types::WhisperModel {
        match size {
            WhisperModelSize::Tiny => voice_core::types::WhisperModel::Tiny,
            WhisperModelSize::Base => voice_core::types::WhisperModel::Base,
            WhisperModelSize::Small => voice_core::types::WhisperModel::Small,
            WhisperModelSize::Medium => voice_core::types::WhisperModel::Medium,
        }
    }

    /// OpenAI Whisper API 识别
    async fn transcribe_openai(
        credential: &AsrCredentialEntry,
        audio_data: &[u8],
        sample_rate: u32,
    ) -> Result<String, String> {
        let config = credential.openai_config.as_ref().ok_or("OpenAI 配置缺失")?;
        let audio = Self::build_audio_data(audio_data, sample_rate)?;

        let mut client = OpenAIWhisperClient::new(config.api_key.clone());
        if let Some(base_url) = config.base_url.clone() {
            client = client.with_host(base_url);
        }
        if !credential.language.is_empty() {
            client = client.with_language(credential.language.clone());
        }

        let result = client
            .transcribe(&audio)
            .await
            .map_err(|e| format!("OpenAI Whisper 识别失败: {e}"))?;

        Ok(result.text)
    }

    /// 百度语音识别
    async fn transcribe_baidu(
        credential: &AsrCredentialEntry,
        audio_data: &[u8],
        sample_rate: u32,
    ) -> Result<String, String> {
        let config = credential.baidu_config.as_ref().ok_or("百度配置缺失")?;
        let audio = Self::build_audio_data(audio_data, sample_rate)?;

        let client = BaiduClient::new(config.api_key.clone(), config.secret_key.clone());
        let result = client
            .transcribe(&audio)
            .await
            .map_err(|e| format!("百度识别失败: {e}"))?;

        Ok(result.text)
    }

    /// 讯飞语音识别
    ///
    /// 使用 WebSocket 流式识别，支持实时语音转文字
    async fn transcribe_xunfei(
        credential: &AsrCredentialEntry,
        audio_data: &[u8],
        sample_rate: u32,
    ) -> Result<String, String> {
        let config = credential.xunfei_config.as_ref().ok_or("讯飞配置缺失")?;
        let audio = Self::build_audio_data(audio_data, sample_rate)?;

        // 创建讯飞客户端
        // 讯飞语言代码转换：zh -> zh_cn, en -> en_us
        let xunfei_language = match credential.language.as_str() {
            "zh" => "zh_cn".to_string(),
            "en" => "en_us".to_string(),
            other => other.to_string(),
        };

        let client = XunfeiClient::new(
            config.app_id.clone(),
            config.api_key.clone(),
            config.api_secret.clone(),
        )
        .with_language(xunfei_language);

        let result = client
            .transcribe(&audio)
            .await
            .map_err(|e| format!("讯飞识别失败: {e}"))?;

        Ok(result.text)
    }

    /// 将 PCM 字节构造成 voice-core 的 AudioData
    fn build_audio_data(audio_data: &[u8], sample_rate: u32) -> Result<AudioData, String> {
        let audio = AudioData::from_pcm16le_bytes(audio_data, sample_rate, 1);
        if audio.samples.is_empty() {
            return Err("音频数据为空".to_string());
        }

        Ok(audio)
    }
}
